/*
 * Copyright Institute for Theoretical Physics, ETH Zurich 2015.
 * Distributed under the Boost Software License, Version 1.0.
 *
 * Permission is hereby granted, free of charge, to any person or organization
 * obtaining a copy of the software and accompanying documentation covered by
 * this license (the "Software") to use, reproduce, display, distribute,
 * execute, and transmit the Software, and to prepare derivative works of the
 * Software, and to permit third-parties to whom the Software is furnished to
 * do so, all subject to the following:
 *
 * The copyright notices in the Software and this entire statement, including
 * the above license grant, this restriction and the following disclaimer,
 * must be included in all copies of the Software, in whole or in part, and
 * all derivative works of the Software, unless such copies or derivative
 * works are solely in the form of machine-executable object code generated by
 * a source language processor.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
 * SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
 * FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */

#ifndef BIND
#define BIND
#ifndef NDEBUG
#define NDEBUG
#define BIND_NO_DEBUG
#endif
// {{{ required packages
#if defined(BIND_REQUIRE_MPI) && !defined(MPI_VERSION)
#pragma message("Warning: mpi.h is required but wasn't included.")
#include <mpi.h>
#endif
#if defined(BIND_REQUIRE_CUDA) && !defined(CUDART_VERSION)
#pragma message("Warning: cuda_runtime.h is required but wasn't included.")
#include <cuda_runtime.h>
#endif
// }}}
// {{{ system includes
#include <complex>
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>
#include <string>
#include <cstring>
#include <limits>
#include <vector>
#include <stack>
#include <set>
#include <map>
#include <list>
#include <memory.h>
#include <stdarg.h>
#include <ctype.h>
#include <iostream>
#include <fstream>
#include <sys/time.h>
#include <algorithm>
#include <execinfo.h>
#include <stdexcept>
#include <type_traits>
#include <functional>
#include <utility>
#include <atomic>
#include <tuple>
#include <sys/mman.h>
#include <chrono>
#include <cilk/cilk.h>
#include <cilk/cilk_api.h>
// }}}
// {{{ utils package
#ifndef BIND_UTILS_RSS
#define BIND_UTILS_RSS


#if defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) && defined(__MACH__))
#include <unistd.h>
#include <sys/resource.h>

#if defined(__APPLE__) && defined(__MACH__)
#include <mach/mach.h>

#elif (defined(_AIX) || defined(__TOS__AIX__)) || (defined(__sun__) || defined(__sun) || defined(sun) && (defined(__SVR4) || defined(__svr4__)))
#include <fcntl.h>
#include <procfs.h>

#elif defined(__linux__) || defined(__linux) || defined(linux) || defined(__gnu_linux__)
#include <stdio.h>

#endif

#else
#error "Cannot define getPeakRSS( ) or getCurrentRSS( ) for an unknown OS."
#endif

inline size_t getRSSLimit(){
#if defined(__APPLE__) && defined(__MACH__)
    return 0L;

#elif defined(__linux__) || defined(__linux) || defined(linux) || defined(__gnu_linux__)
    long pages = sysconf(_SC_PHYS_PAGES);
    long page_size = sysconf(_SC_PAGE_SIZE);
    return pages * page_size;
#else
    return (size_t)0L;
#endif
    
}

inline size_t getPeakRSS( )
{
#if (defined(_AIX) || defined(__TOS__AIX__)) || (defined(__sun__) || defined(__sun) || defined(sun) && (defined(__SVR4) || defined(__svr4__)))
    struct psinfo psinfo;
    int fd = -1;
    if ( (fd = open( "/proc/self/psinfo", O_RDONLY )) == -1 )
        return (size_t)0L;
    if ( read( fd, &psinfo, sizeof(psinfo) ) != sizeof(psinfo) )
    {
        close( fd );
        return (size_t)0L;
    }
    close( fd );
    return (size_t)(psinfo.pr_rssize * 1024L);

#elif defined(__unix__) || defined(__unix) || defined(unix) || (defined(__APPLE__) && defined(__MACH__))
    struct rusage rusage;
    getrusage( RUSAGE_SELF, &rusage );
#if defined(__APPLE__) && defined(__MACH__)
    return (size_t)rusage.ru_maxrss;
#else
    return (size_t)(rusage.ru_maxrss * 1024L);
#endif

#else
    return (size_t)0L;
#endif
}

inline size_t getCurrentRSS( )
{
#if defined(__APPLE__) && defined(__MACH__)
    struct mach_task_basic_info info;
    mach_msg_type_number_t infoCount = MACH_TASK_BASIC_INFO_COUNT;
    if ( task_info( mach_task_self( ), MACH_TASK_BASIC_INFO,
                (task_info_t)&info, &infoCount ) != KERN_SUCCESS )
        return (size_t)0L;
    return (size_t)info.resident_size;

#elif defined(__linux__) || defined(__linux) || defined(linux) || defined(__gnu_linux__)
    long rss = 0L;
    FILE* fp = NULL;
    if ( (fp = fopen( "/proc/self/statm", "r" )) == NULL )
        return (size_t)0L;
    if ( fscanf( fp, "%*s%ld", &rss ) != 1 )
    {
        fclose( fp );
        return (size_t)0L;
    }
    fclose( fp );
    return (size_t)rss * (size_t)sysconf( _SC_PAGESIZE);

#else
    return (size_t)0L;
#endif
}

#endif
#ifndef BIND_UTILS_INDEX_SEQUENCE
#define BIND_UTILS_INDEX_SEQUENCE

namespace bind {

    template<size_t... Indices>
    struct index_sequence {
        template<size_t N>
        using append = index_sequence<Indices..., N>;
    };

    template<size_t Size>
    struct make_index_sequence_impl {
        typedef typename make_index_sequence_impl<Size-1>::type::template append<Size-1> type;
    };

    template<>
    struct make_index_sequence_impl<0u> {
        typedef index_sequence<> type;
    };

    template<size_t Size>
    using make_index_sequence = typename make_index_sequence_impl<Size>::type;

}

#endif

#ifndef BIND_UTILS_IO
#define BIND_UTILS_IO

namespace bind { namespace utils {

    class funneled_io {
    public:
        funneled_io() : nullio("/dev/null"), latch(NULL) { }
       ~funneled_io(){
            disable();
        }
        void enable(){
            latch = std::cout.rdbuf();
            std::cout.rdbuf(nullio.rdbuf());
        }
        void disable(){
            if(!latch) return;
            std::cout.rdbuf(latch);
            latch = NULL;
        }
    private:
        std::ofstream nullio;
        std::streambuf* latch;
    };

} }

#endif

#ifndef BIND_UTILS_MUTEX
#define BIND_UTILS_MUTEX

namespace bind { 

    class guard_once {
    public:
        guard_once() : once(false) { }
        bool operator()(){ if(!once){ once = true; return true; } return false; }
    private:
        bool once;
    };

    template <typename M>
    class guard {
    private:
        M& mtx;
        guard(const guard &);
        void operator= (const guard &);
    public:
        explicit guard(M& nmtx) : mtx(nmtx){ mtx.lock(); }
        ~guard(){ mtx.unlock(); }
    };

    class mutex {
    private:
        pthread_mutex_t m;
    public:
        mutex(mutex const&) = delete;
        mutex& operator= (mutex const&) = delete;

        mutex(){
            int const res = pthread_mutex_init(&m,NULL);
            assert(res == 0);
        }
       ~mutex(){
            int ret;
            do{ ret = pthread_mutex_destroy(&m);
            } while(ret == EINTR);
        }
        void lock(){
            int res;
            do{ res = pthread_mutex_lock(&m);
            }while (res == EINTR);
            assert(res == 0);
        }
        void unlock(){
            int res;
            do{ res = pthread_mutex_unlock(&m);
            } while(res == EINTR);
            assert(res == 0);
        }
        bool try_lock(){
            int res;
            do{ res = pthread_mutex_trylock(&m);
            } while(res == EINTR);
            if(res == EBUSY) return false;
            return !res;
        }
    };
}

#endif

#ifndef BIND_UTILS_RANK_T
#define BIND_UTILS_RANK_T

namespace bind {
    #if 0
    class rank_t {
    public:
        rank_t(){}
        rank_t(int r) : rank(r) {}
        bool operator != (const rank_t& other) const { return (rank != other.rank); }
        bool operator == (const rank_t& other) const { return (rank == other.rank); }
        bool operator <  (const rank_t& other) const { return (rank < other.rank);  }
        bool operator >  (const rank_t& other) const { return (rank > other.rank);  }
        int& toint() const { return rank; }
        mutable int rank; // mutable due to MPI
    };
    #else
    typedef int rank_t;
    #endif
}

#endif
// }}}
// {{{ memory package

#ifndef BIND_MEMORY_TYPES
#define BIND_MEMORY_TYPES

namespace bind { namespace memory {
    // {{{ types lookup detail
    namespace detail {
        template<int N, int Limit>
        constexpr int lower_bound(){
            return (N > Limit ? N : Limit);
        }

        template<int Offset>
        struct checked_get { static constexpr int value = Offset; };

        template<>
        struct checked_get< -1 > { /* type not found */ };

        template<class T, class Tuple, int I = std::tuple_size<Tuple>::value>
        constexpr int find_type(){
            return I ? std::is_same< typename std::tuple_element<lower_bound<I-1,0>(),Tuple>::type, T >::value ? I-1 : find_type<T,Tuple,lower_bound<I-1,0>()>()
                     : -1;
        }
    }
    // }}}

    namespace cpu {
        class standard;
        class bulk;
    }
    namespace gpu {
        class pinned;
        class standard;
    }

    struct types {
        typedef int id_type;
        typedef std::tuple< memory::cpu::bulk,
                            memory::cpu::standard,
                            memory::gpu::pinned,
                            memory::gpu::standard
                            > list;

        template<typename T>
        using id = detail::checked_get< detail::find_type<T,list>() >;

        struct cpu {
            static constexpr id_type bulk = id<memory::cpu::bulk>::value;
            static constexpr id_type standard = id<memory::cpu::standard>::value;
        };
        struct gpu {
            static constexpr id_type pinned = id<memory::gpu::pinned>::value;
            static constexpr id_type standard = id<memory::gpu::standard>::value;
        };
        static constexpr id_type none = std::tuple_size<list>::value;
    };
} }

#endif

#ifndef BIND_MEMORY_FACTORY
#define BIND_MEMORY_FACTORY

namespace bind { namespace memory {

    template<size_t S>
    class private_factory {
    public:
        private_factory(){
            this->buffers.push_back(std::malloc(S));
            this->buffer = &this->buffers[0];
        }
       ~private_factory(){
            for(int i = 0; i < buffers.size(); i++) 
                std::free(this->buffers[i]);
        }
        void* provide(){
            void* chunk = *buffer;
            if(chunk == buffers.back()){
                buffers.push_back(std::malloc(S));
                buffer = &buffers.back();
            }else
                buffer++;
            return chunk;
        }
        void reset(){
            buffer = &buffers[0];
        }
        size_t size(){
            return (buffer - &buffers[0]);
        }
    private:
        std::vector<void*> buffers;
        void** buffer;
    };

    template<size_t S>
    class factory {
    public:
        typedef bind::mutex mutex;
        typedef bind::guard<mutex> guard;
    private:
        factory(const factory&) = delete;
        factory& operator=(const factory&) = delete;
        factory(){
            this->buffers.push_back(std::malloc(S));
            this->buffer = &this->buffers[0];
        }
    public:
        static factory& instance(){
            static factory singleton; return singleton;
        }
        static void* provide(){
            factory& s = instance();
            guard g(s.mtx);
            void* chunk;

            chunk = *s.buffer;
            if(*s.buffer == s.buffers.back()){
                s.buffers.push_back(std::malloc(S));
                s.buffer = &s.buffers.back();
            }else
                s.buffer++;

            return chunk;
        }
        static void deallocate(){
            factory& s = instance();
            for(int i = 1; i < s.buffers.size(); i++) std::free(s.buffers[i]);
            s.buffers.resize(1);
        }
        static void reset(){
            factory& s = instance();
            s.buffer = &s.buffers[0];
        }
        static size_t size(){
            factory& s = instance();
            return (s.buffer - &s.buffers[0]);
        }
    private:
        mutex mtx;
        std::vector<void*> buffers;
        void** buffer;
    };

} }

#endif

#ifndef BIND_MEMORY_REGION
#define BIND_MEMORY_REGION

namespace bind { namespace memory {

    constexpr size_t aligned_64(size_t size){ return 64 * (size_t)((size+63)/64); }
    template<size_t S> constexpr size_t aligned_64(){ return 64 * (size_t)((S+63)/64); }

    template<size_t S, class Factory>
    class serial_region {
    public:
        serial_region(){
            this->buffer = NULL;
            this->iterator = (char*)this->buffer+S;
        }
        void realloc(){
            this->buffer = Factory::provide();
            this->iterator = (char*)this->buffer;
        }
        void* malloc(size_t sz){
            if(((size_t)iterator + sz - (size_t)this->buffer) >= S) realloc();
            void* m = (void*)iterator;
            iterator += aligned_64(sz);
            return m;
        }
        void reset(){
            this->iterator = (char*)this->buffer+S;
        }
    protected:
        void* buffer;
        char* iterator;
    };

    template<size_t S, class Factory>
    class private_region : public serial_region<S,Factory> {
    public:
        typedef serial_region<S,Factory> base;
        void* malloc(size_t sz){
            if(((size_t)this->iterator + sz - (size_t)this->buffer) >= S){
                this->buffer = pool.provide();
                this->iterator = (char*)this->buffer;
            }
            void* m = (void*)this->iterator;
            this->iterator += aligned_64(sz);
            return m;
        }
        void reset(){
            base::reset();
            pool.reset();
        }
    private:
        Factory pool;
    };

    template<size_t S, class Factory>
    class region : public serial_region<S,Factory> {
    public:
        typedef bind::mutex mutex;
        typedef bind::guard<mutex> guard;
        typedef serial_region<S,Factory> base;

        void* malloc(size_t sz){
            guard g(this->mtx);
            return base::malloc(sz);
        }
    private:
        mutex mtx;
    };

} }

#endif
    // {{{ memory::cpu package

#ifndef BIND_MEMORY_CPU_BULK
#define BIND_MEMORY_CPU_BULK

#ifndef BIND_INSTR_BULK_CHUNK
#define BIND_INSTR_BULK_CHUNK     16777216 // 16 MB
#endif
#ifndef BIND_DATA_BULK_CHUNK
#define BIND_DATA_BULK_CHUNK      67108864 // 64 MB
#endif
#ifndef BIND_COMM_BULK_CHUNK
#define BIND_COMM_BULK_CHUNK      67108864 // 64 MB
#endif

namespace bind { namespace memory { namespace cpu {

    struct bulk {
        static constexpr int type = types::cpu::bulk;
        bulk(const bulk&) = delete;
        bulk& operator=(const bulk&) = delete;
    protected:
        bulk() = default;
    };

} } }

#endif

#ifndef BIND_MEMORY_CPU_DATA_BULK
#define BIND_MEMORY_CPU_DATA_BULK

#define DATA_BULK_LIMIT 10

namespace bind { namespace memory { namespace cpu {

    class data_bulk : public bulk {
        data_bulk(){
            this->soft_limit = DATA_BULK_LIMIT * ((double)getRSSLimit() / BIND_DATA_BULK_CHUNK / 100);
        }
    public:
        static data_bulk& instance(){
            static data_bulk singleton; return singleton;
        }
        static void* soft_malloc(size_t s){
            if(instance().soft_limit < factory<BIND_DATA_BULK_CHUNK>::size() || s > BIND_DATA_BULK_CHUNK) return NULL;
            return instance().memory.malloc(s);
        }

        static void drop(){
            instance().memory.reset();
            if(instance().soft_limit < factory<BIND_DATA_BULK_CHUNK>::size())
                factory<BIND_DATA_BULK_CHUNK>::deallocate();
            factory<BIND_DATA_BULK_CHUNK>::reset();
        }
    private:
        region<BIND_DATA_BULK_CHUNK, factory<BIND_DATA_BULK_CHUNK> > memory;
        size_t soft_limit;
    };

} } }

#undef DATA_BULK_LIMIT
#endif

#ifndef BIND_MEMORY_CPU_COMM_BULK
#define BIND_MEMORY_CPU_COMM_BULK

#define COMM_BULK_LIMIT 20

namespace bind { namespace memory { namespace cpu {

    class comm_bulk : public bulk {
        comm_bulk(){
            this->soft_limit = COMM_BULK_LIMIT * ((double)getRSSLimit() / BIND_COMM_BULK_CHUNK / 100);
        }
    public:
        static comm_bulk& instance(){
            static comm_bulk singleton; return singleton;
        }
        template<size_t S> static void* malloc()        { return instance().memory.malloc(S); }
                           static void* malloc(size_t s){ return instance().memory.malloc(s); }
        static void drop(){
            instance().memory.reset();
            if(instance().soft_limit < factory<BIND_COMM_BULK_CHUNK>::size())
                factory<BIND_COMM_BULK_CHUNK>::deallocate();
            factory<BIND_COMM_BULK_CHUNK>::reset();
        }
    private:
        size_t soft_limit;
        region<BIND_COMM_BULK_CHUNK, factory<BIND_COMM_BULK_CHUNK> > memory;
    };

} } }

#undef COMM_BULK_LIMIT
#endif

#ifndef BIND_MEMORY_CPU_INSTR_BULK
#define BIND_MEMORY_CPU_INSTR_BULK

namespace bind { namespace memory { namespace cpu {

    struct instr_bulk {
        template<class T>
        class allocator {
        public:
            typedef T value_type;
            template <class U> struct rebind { typedef allocator<U> other; };
            allocator() throw() { }
            allocator(const allocator&) throw() { }
            template<typename U> allocator(const allocator<U>&) throw() { }
           ~allocator() throw() { }
            static void deallocate(T* p, size_t n){ }
            static T* allocate(size_t n){
                return (T*)instr_bulk::malloc(n*sizeof(T));
            }
        };

        static instr_bulk& instance(){
            static instr_bulk singleton; return singleton;
        }
        template<size_t S> 
        static void* malloc(){
            return malloc(S);
        }
        static void* malloc(size_t s){
            return instance().impl.malloc(s);
        }
        static void drop(){
            instance().impl.reset();
        }
    private:
        memory::private_region<BIND_INSTR_BULK_CHUNK, 
                               memory::private_factory<BIND_INSTR_BULK_CHUNK> 
                              > impl;
    };

} } }

#endif


#ifndef BIND_MEMORY_CPU_STANDARD
#define BIND_MEMORY_CPU_STANDARD

namespace bind { namespace memory { namespace cpu {

    struct standard {
        static constexpr int type = types::cpu::standard;

        template<size_t S> static void* malloc(){ return std::malloc(S);   }
        template<size_t S> static void* calloc(){ return std::calloc(1,S); }

        static void* malloc(size_t sz){ return std::malloc(sz); }
        static void* calloc(size_t sz){ return std::calloc(1,sz); }

        static void free(void* ptr){ std::free(ptr);  }
    };

} } }

#endif

#ifndef BIND_MEMORY_CPU_NEW
#define BIND_MEMORY_CPU_NEW

namespace bind { namespace memory { namespace cpu {

    template<class T>
    class use_fixed_new {
    public:
        void* operator new (size_t sz){ assert(sz == sizeof(T)); return memory::cpu::standard::malloc<sizeof(T)>(); }
        void operator delete (void* ptr){ memory::cpu::standard::free(ptr); }
    };

    template<class T>
    class use_bulk_new {
    public:
        void* operator new (size_t sz){ assert(sz == sizeof(T)); return memory::cpu::instr_bulk::malloc<sizeof(T)>(); }
        void operator delete (void* ptr){ }
    };

} } }

#endif
    // }}}
    // {{{ memory::gpu package
    #ifdef CUDART_VERSION

#ifndef BIND_MEMORY_GPU_PINNED
#define BIND_MEMORY_GPU_PINNED

namespace bind { namespace memory { namespace gpu {

    struct pinned {
	static constexpr int type = types::gpu::pinned;

	template<size_t S> static void* malloc(){ void* ptr; cudaMallocHost(&ptr, S); return ptr; }
	template<size_t S> static void* calloc(){ void* ptr = malloc<S>(); memset(ptr, 0, S); return ptr; }

	static void* malloc(size_t sz){ void* ptr; cudaMallocHost(&ptr, sz); return ptr; }
	static void* calloc(size_t sz){ void* ptr = malloc(sz); memset(ptr, 0, sz); return ptr; }

	static void free(void* ptr){ cudaFreeHost(ptr); }
    };

} } }

#endif

#ifndef BIND_MEMORY_GPU_STANDARD
#define BIND_MEMORY_GPU_STANDARD

namespace bind { namespace memory { namespace gpu {

    struct standard {
        static constexpr int type = types::gpu::standard;

        template<size_t S> static void* malloc(){ void* ptr; cudaMalloc(&ptr, S); return ptr; }
        template<size_t S> static void* calloc(){ void* ptr = malloc<S>(); cudaMemset(ptr, 0, S); return ptr; }

        static void* malloc(size_t sz){ void* ptr; cudaMalloc(&ptr, sz); return ptr; }
        static void* calloc(size_t sz){ void* ptr = malloc(sz); cudaMemset(ptr, 0, sz); return ptr; }

        static void free(void* ptr){ cudaFree(ptr); }
    };

} } }

#endif
    #endif
    // }}}

#ifndef BIND_MEMORY_DESCRIPTOR
#define BIND_MEMORY_DESCRIPTOR

namespace bind { namespace memory {

    class descriptor;

    template<class Device>
    struct hub {
        static bool conserves(descriptor& c, descriptor& p){
            return (c.tmp || p.type == types::cpu::standard || c.type == types::cpu::bulk);
        }
        static void* malloc(descriptor& c){
            if(c.tmp || c.type == types::cpu::bulk){
                void* ptr = cpu::data_bulk::soft_malloc(c.extent);
                if(ptr){ c.type = types::cpu::bulk; return ptr; }
            }
            c.type = types::cpu::standard;
            return cpu::standard::malloc(c.extent);
        }
        static void memset(descriptor& desc, void* ptr){
            std::memset(ptr, 0, desc.extent);
        }
        static void memcpy(descriptor& dst_desc, void* dst, descriptor& src_desc, void* src){
            std::memcpy(dst, src, src_desc.extent);
        }
    };

    struct descriptor {
        template<class Device> friend struct hub;
        descriptor(size_t e, types::id_type t = types::none) : extent(e), type(t), tmp(false) {}

        void free(void* ptr){
            if(!ptr) return;
            switch(type){
                case types::none: return;
                case types::cpu::standard: cpu::standard::free(ptr); break;
                default: return;
            }
            type = types::none;
        }
        template<class Memory>
        void* hard_malloc(){
            type = Memory::type;
            return Memory::malloc(extent);
        }
        template<class Device>
        void* malloc(){
            return hub<Device>::malloc(*this);
        }
        template<class Device>
        void* calloc(){
            void* m = hub<Device>::malloc(*this);
            hub<Device>::memset(*this, m);
            return m;
        }
        template<class Device>
        void memcpy(void* dst, void* src, descriptor& src_desc){
            hub<Device>::memcpy(*this, dst, src_desc, src);
        }
        template<class Device>
        bool conserves(descriptor& p){
            return hub<Device>::conserves(*this, p);
        }
        void reuse(descriptor& d){
            type = d.type;
            d.type = types::none;
        }
        void temporary(bool t){
            tmp = t;
        }
    public:
        const size_t extent;
    private:
        types::id_type type;
        bool tmp;
    };

} }

#endif
// }}}
// {{{ model package

#ifndef BIND_MODEL_LOCALITY
#define BIND_MODEL_LOCALITY

namespace bind {
    enum class locality { remote, local, common };
}

#endif

#ifndef BIND_MODEL_FUNCTOR
#define BIND_MODEL_FUNCTOR

namespace bind { namespace model {
    
    class functor {
        typedef memory::cpu::instr_bulk::allocator<functor*> allocator_type;
    public:
        virtual void invoke() = 0;
        virtual bool ready() = 0;
        void queue(functor* d){ deps.push_back(d); }
        std::vector<functor*, allocator_type> deps;
        void* arguments[1]; // note: trashing the vtptr of derived object
    };

} }

#endif

#ifndef BIND_MODEL_REVISION
#define BIND_MODEL_REVISION

namespace bind { namespace model {

    class revision : public memory::cpu::use_fixed_new<revision> {
    public:
        revision(size_t extent, functor* g, locality l, rank_t owner)
        : spec(extent), generator(g), state(l), 
          data(NULL), users(0), owner(owner),
          crefs(1)
        {
        }

        void embed(void* ptr){
            data = ptr;
        }
        void reuse(revision& r){
            data = r.data;
            spec.reuse(r.spec);
        }
        void use(){
            ++users;
        }
        void release(){
            --users;
        }
        void complete(){
            generator = NULL;
        }
        void invalidate(){
            data = NULL;
        }
        bool locked() const {
            return (users != 0);
        }
        bool locked_once() const {
            return (users == 1);
        }
        bool valid() const {
            return (data != NULL);
        }
        bool referenced() const {
            return (crefs != 0);
        }
        void protect(){
            crefs++;
            if(valid() || state == locality::remote) return;
            if(crefs == 1) spec.temporary(false);
        }
        void weaken(){
            crefs--;
            if(valid() || state == locality::remote) return;
            if(!crefs) spec.temporary(true);
        }
        std::atomic<functor*> generator;
        void* data;
        rank_t owner;
        std::atomic<int> users;
        const locality state;
        std::pair<size_t, functor*> assist;
        memory::descriptor spec;
    private:
        int crefs;
    };

    inline bool local(const revision* r){
        return (r->state == locality::local);
    }
    
    inline bool remote(const revision* r){
        return (r->state == locality::remote);
    }
    
    inline bool common(const revision* r){
        return (r->state == locality::common);
    }
    
    inline rank_t owner(const revision* r){
        return r->owner;
    }

} }

#endif

#ifndef BIND_MODEL_HISTORY
#define BIND_MODEL_HISTORY

// revision tracking mechanism (target selector)
namespace bind { namespace model {

    class history : public memory::cpu::use_fixed_new<history> {
    public:
        history(size_t size) : current(NULL), extent(memory::aligned_64(size)) { }
        void init_state(rank_t owner){
            revision* r = new revision(extent, NULL, locality::common, owner);
            this->current = r;
        }
        template<locality L> void add_state(functor* gen, rank_t owner){
            revision* r = new revision(extent, gen, L, owner);
            this->current = r;
        }
        revision* back() const {
            return this->current;
        }
        bool weak() const {
            return (this->back() == NULL);
        }
        revision* current;
        size_t extent;
    };

} }

#endif

#ifndef BIND_MODEL_ANY
#define BIND_MODEL_ANY

namespace bind { namespace model {

    template<typename T>
    constexpr size_t sizeof_any(){
        return (2*sizeof(void*) + sizeof(size_t) + memory::aligned_64<sizeof(T)>());
    }

    class any {
    public:
        // WARNING: the correct allocation of sizeof_any required
        void* operator new (size_t, void* place){ return place; }
        void operator delete (void*, void*){}

        template<typename T>
        any(T val) : size(memory::aligned_64<sizeof(T)>()) { *this = val; }
        template<typename T> void operator = (T val){ *(T*)&value = val; }
        template<typename T> operator T& (){ return *(T*)&value;  }
        void complete(){ generator = NULL; }

        std::atomic<functor*> generator;
        any* origin;
        size_t size;
        int value;
    };

} }

#endif
// }}}
// {{{ transport package (requires :model)
#ifdef MPI_VERSION
#define BIND_CHANNEL_NAME mpi

#ifndef BIND_TRANSPORT_MPI_GROUP
#define BIND_TRANSPORT_MPI_GROUP

namespace bind { namespace transport { namespace mpi {

    struct group {
        group(MPI_Comm parent) : mpi_comm(parent)
        {
            MPI_Comm_group(this->mpi_comm,  &this->mpi_group);
            MPI_Group_size(this->mpi_group, &this->size);
            MPI_Group_rank(this->mpi_group, &this->rank);
        }
        int rank;
        int size;
        MPI_Comm  mpi_comm;
        MPI_Group mpi_group;
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_TREE
#define BIND_TRANSPORT_MPI_TREE

#define BOUNDARY_OVERFLOW -1

namespace bind {

    template<typename T>
    class binary_tree {
    public:
        binary_tree(size_t N) : tree(N), length(N) {
            entry_point = N/2;
            generate(tree, N, N);
            normalize();
            check();
        }
        T generate(std::vector<std::pair<T,T> >& tree, int N, int L, int start = 0){
            if(N <= 0 || start >= L) return BOUNDARY_OVERFLOW;
            tree[start+N/2] = std::make_pair(generate(tree, N/2, L, start), 
                                             generate(tree, N-N/2-1, L, start+N/2+1));
            return start+N/2;
        }
        void normalize(){
            std::vector<std::pair<T,T> > normalized(length);
            for(int i = 0; i < length; i++){
                normalized[i] = tree[(i+entry_point)%length];
                if(normalized[i].first  != BOUNDARY_OVERFLOW) normalized[i].first  = (normalized[i].first  - entry_point + length) % length;
                if(normalized[i].second != BOUNDARY_OVERFLOW) normalized[i].second = (normalized[i].second - entry_point + length) % length;
            }
            std::swap(tree, normalized);
            entry_point = 0;
        }
        void check(){
            std::vector<bool> states(length);
            for(int i = 0; i < length; i++){
                if(tree[i].first  != BOUNDARY_OVERFLOW) states[tree[i].first]  = true;
                if(tree[i].second != BOUNDARY_OVERFLOW) states[tree[i].second] = true;
            }
            for(int i = 0; i < length; i++){
                if(!states[i] && i != entry_point) throw std::runtime_error("Error: no route to host");
            }
        }
        std::pair<T,T> operator[](int i) const {
            return tree[i];
        }
    private:
        std::vector<std::pair<T,T> > tree;
        int entry_point;
        size_t length;
    };
}

#undef BOUNDARY_OVERFLOW
#endif

#ifndef BIND_TRANSPORT_MPI_CHANNEL_H
#define BIND_TRANSPORT_MPI_CHANNEL_H

namespace bind { namespace transport { namespace mpi {

    class request_impl;
    template<class T> class collective;

    static void recv_impl(request_impl* r);
    static void send_impl(request_impl* r);
    static bool test_impl(request_impl* r);

    class channel {
    public:
        typedef typename model::revision block_type;
        typedef typename model::any scalar_type;
        template<class T> using collective_type = collective<T>;
        struct mount {
            mount(); 
           ~mount();
            std::vector<binary_tree<rank_t>*> trees;
            std::vector<rank_t> circle;
            int tag_ub;
            int sid;
            int self;
            int np;
        };
        static mount& setup(){ 
            static mount m; 
            return m; 
        }
        channel();
        size_t dim() const;
        static void barrier();
        collective<block_type>* get(block_type& r);
        collective<block_type>* set(block_type& r);
        collective<scalar_type>* bcast(scalar_type& v, rank_t root);
        collective<scalar_type>* bcast(scalar_type& v);
        rank_t rank;
        group* world;
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_REQUEST_H
#define BIND_TRANSPORT_MPI_REQUEST_H

namespace bind { namespace transport { namespace mpi {

    class request_impl : public memory::cpu::use_bulk_new<request_impl> {
    public:
        request_impl(void(*impl)(request_impl*), typename channel::scalar_type& v, rank_t target, int tag = 0);
        request_impl(void(*impl)(request_impl*), typename channel::block_type& r, rank_t target, int tag = 0);
        inline bool operator()();
        void* data;
        int extent;
        int target; // MPI_INT
        MPI_Request mpi_request;
        void(*impl)(request_impl*);
        bool once;
        int tag;
    };

    class request {
        typedef memory::cpu::instr_bulk::allocator<request_impl*> allocator_type;
    public:
        bool operator()();
        void operator &= (request_impl* r);
        void operator += (request_impl* r);
    private:
        std::vector<request_impl*,allocator_type> primaries;
        std::vector<request_impl*,allocator_type> callbacks;
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_COLLECTIVE_H
#define BIND_TRANSPORT_MPI_COLLECTIVE_H

namespace bind { namespace transport { namespace mpi {

    template<typename T>
    class bcast {
        typedef memory::cpu::instr_bulk::allocator<int> allocator_type;
    public:
        void dispatch();
        bcast(T& o, rank_t root) : object(o), root(root), self(0) {}
    private:
        template<class C> friend class collective;
        T& object;
        std::vector<int,allocator_type> tags;
        rank_t root;
        int self;
        int size;
        rank_t* list;
        request impl; 
        guard_once once;
    };

    template<class T> class collective {};

    template<>
    class collective<typename channel::block_type> : public bcast<typename channel::block_type>, 
                                                     public memory::cpu::use_bulk_new<collective<typename channel::block_type> > {
        typedef memory::cpu::instr_bulk::allocator<int> allocator_type;
    public:
        collective(typename channel::block_type& r, rank_t root);
        void append(rank_t rank);
        bool involved();
        bool test();
    private:
        std::vector<bool,allocator_type> states;
        std::vector<rank_t,allocator_type> tree;
    };

    template<>
    class collective<typename channel::scalar_type> : public bcast<typename channel::scalar_type>, 
                                                      public memory::cpu::use_bulk_new<collective<typename channel::scalar_type> > {
    public:
        collective(typename channel::scalar_type& v, rank_t root);
        bool test();
    };

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_REQUEST_HPP
#define BIND_TRANSPORT_MPI_REQUEST_HPP

namespace bind { namespace transport { namespace mpi {

    // type information required //
    inline request_impl::request_impl(void(*impl)(request_impl*), typename channel::scalar_type& v, rank_t target, int tag)
    : extent(v.size/sizeof(double)), 
      data(&v.value),
      target(target),
      impl(impl),
      tag(tag),
      once(false)
    {
    }
    // type information required //
    inline request_impl::request_impl(void(*impl)(request_impl*), typename channel::block_type& r, rank_t target, int tag)
    : extent(r.spec.extent/sizeof(double)), 
      data(r.data),
      target(target),
      impl(impl),
      tag(tag),
      once(false)
    {
    }
    inline bool request_impl::operator()(){
        if(!once){ impl(this); once = true; }
        return test_impl(this);
    }

    inline bool request::operator()(){
        int length = primaries.size();
        for(int i = 0; i < length; i++){
            if(!(*primaries[i])()) return false;
        }
        primaries.clear();
        length = callbacks.size();
        for(int i = 0; i < length; i++){
            if(!(*callbacks[i])()) return false;
        }
        return true;
    }
    inline void request::operator &= (request_impl* r){
        primaries.push_back(r);
    }
    inline void request::operator += (request_impl* r){
        callbacks.push_back(r);
    }

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_CHANNEL_HPP
#define BIND_TRANSPORT_MPI_CHANNEL_HPP

namespace bind { namespace transport { namespace mpi {

    inline void recv_impl(request_impl* r){
        MPI_Irecv(r->data, r->extent, MPI_DOUBLE, r->target, r->tag, MPI_COMM_WORLD, &r->mpi_request);
    }
    inline void send_impl(request_impl* r){
        MPI_Isend(r->data, r->extent, MPI_DOUBLE, r->target, r->tag, MPI_COMM_WORLD, &r->mpi_request);
    }
    inline bool test_impl(request_impl* r){
        int f = 0; MPI_Test(&r->mpi_request, &f, MPI_STATUS_IGNORE); return f;
    }

    inline channel::mount::mount(){
        int *ub, flag, level, zero = 0;
        MPI_Init_thread(&zero, NULL, MPI_THREAD_FUNNELED, &level); 
        if(level != MPI_THREAD_FUNNELED) throw std::runtime_error("Error: Wrong threading level");
        MPI_Comm_size(MPI_COMM_WORLD, &np);
        MPI_Comm_rank(MPI_COMM_WORLD, &self);
        MPI_Attr_get(MPI_COMM_WORLD, MPI_TAG_UB, &ub, &flag);
        this->tag_ub = flag ? *ub : 32767;
        this->sid = 1;
        
        trees.resize(2); // 0,1 are empty
        for(int i = 2; i <= np; i++)  trees.push_back(new binary_tree<rank_t>(i));
        for(int i = 0; i < 2*np; i++) circle.push_back(i % np);
    }

    inline channel::mount::~mount(){
        MPI_Finalize();
    }

    inline channel::channel(){
        channel::setup(); // making sure MPI is initialised
        this->world = new group(MPI_COMM_WORLD);
        this->rank = this->world->rank;
    }

    inline void channel::barrier(){
        MPI_Barrier(MPI_COMM_WORLD);
    }

    inline size_t channel::dim() const {
        return this->world->size;
    }

    inline collective<typename channel::scalar_type>* channel::bcast(scalar_type& v, rank_t root){
        return new collective<scalar_type>(v, root);
    }

    inline collective<typename channel::scalar_type>* channel::bcast(scalar_type& v){
        return new collective<scalar_type>(v, rank);
    }

    inline collective<typename channel::block_type>* channel::get(block_type& r){
        return new collective<block_type>(r, r.owner);
    }

    inline collective<typename channel::block_type>* channel::set(block_type& r){
        return new collective<block_type>(r, rank);
    }

} } }

#endif

#ifndef BIND_TRANSPORT_MPI_COLLECTIVE_HPP
#define BIND_TRANSPORT_MPI_COLLECTIVE_HPP

#define BOUNDARY_OVERFLOW -1

namespace bind { namespace transport { namespace mpi {

    namespace detail {
        inline int self(){
            return channel::setup().self;
        }
        inline int num_procs(){
            return channel::setup().np;
        }
        inline int generate_sid(){
            return (++channel::setup().sid %= channel::setup().tag_ub);
        }
    }

    template<typename T>
    inline void bcast<T>::dispatch(){
        std::pair<rank_t,rank_t> lr = (*channel::setup().trees[size])[self];
        if(!self){ // self == root
            if(lr.first  != BOUNDARY_OVERFLOW) impl &= new request_impl(send_impl, object, list[lr.first],  tags[lr.first]);
            if(lr.second != BOUNDARY_OVERFLOW) impl &= new request_impl(send_impl, object, list[lr.second], tags[lr.second]);
        }else{
            impl &= new request_impl(recv_impl, object, MPI_ANY_SOURCE, tags[self]);
            if(lr.first  != BOUNDARY_OVERFLOW) impl += new request_impl(send_impl, object, list[lr.first],  tags[lr.first]);
            if(lr.second != BOUNDARY_OVERFLOW) impl += new request_impl(send_impl, object, list[lr.second], tags[lr.second]);
        }
    }

    inline collective<typename channel::block_type>::collective(typename channel::block_type& r, rank_t root) 
    : bcast<typename channel::block_type>(r, root), states(detail::num_procs()+1) {
        this->tree.push_back(root);
        this->tags.push_back(-1);
    }

    inline void collective<typename channel::block_type>::append(rank_t rank){
        if(!states[rank]){
            states[rank] = true;
            if(states.back()){
                for(int i = this->tags.size(); i <= detail::num_procs(); i++)
                    this->tags.push_back(detail::generate_sid());
                for(int i = 0; i < detail::num_procs(); i++)
                    this->states[i] = true;
            }else{
                if(rank == detail::self()) this->self = tree.size();
                this->tags.push_back(channel::setup().sid);
                this->tree.push_back(rank);
            }
        }
        detail::generate_sid();
    }

    inline bool collective<typename channel::block_type>::involved(){
        return states[detail::self()] || states.back();
    }

    inline bool collective<typename channel::block_type>::test(){
        if(this->once()){
            if(states.back()){
                this->size = detail::num_procs();
                this->list = &channel::setup().circle[root];
                this->self = (size + detail::self() - root) % size;
            }else{
                this->size = tree.size();
                this->list = &tree[0];
            }
            this->dispatch();
        }
        return this->impl();
    }

    inline collective<typename channel::scalar_type>::collective(typename channel::scalar_type& v, rank_t root)
    : bcast<typename channel::scalar_type>(v, root) {
        tags.reserve(detail::num_procs()+1);
        for(int i = 0; i <= detail::num_procs(); i++)
            this->tags.push_back(detail::generate_sid());
    }

    inline bool collective<typename channel::scalar_type>::test(){
        if(this->once()){
            this->size = detail::num_procs();
            this->list = &channel::setup().circle[root];
            this->self = (size + detail::self() - root) % size;
            this->dispatch();
        }
        return this->impl();
    }

} } }

#endif
#else

#ifndef BIND_TRANSPORT_NOP
#define BIND_TRANSPORT_NOP

#define BIND_CHANNEL_NAME nop

namespace bind { namespace transport { namespace nop {

    template<class T> struct collective {
        bool test(){ return true; }
        void append(rank_t rank){}
        bool involved(){ return true; }
    };

    class channel {
    public:
        typedef typename model::revision block_type;
        typedef typename model::any scalar_type;
        template<class T> using collective_type = collective<T>;
        size_t dim() const { return 1; }
        static void barrier(){}
        collective<block_type>* get(block_type& r){ return NULL; }
        collective<block_type>* set(block_type& r){ return NULL; }
        collective<scalar_type>* bcast(scalar_type& v, rank_t root){ return NULL; }
        collective<scalar_type>* bcast(scalar_type& v){ return NULL; }
        static constexpr rank_t rank = 0;
    };

} } }

#endif
#endif
#ifdef CUDART_VERSION

#ifndef BIND_TRANSPORT_CUDA_CHANNEL_HPP
#define BIND_TRANSPORT_CUDA_CHANNEL_HPP

#define NSTREAMS 16

namespace bind { namespace transport { namespace cuda {

    inline cudaError_t checkCuda(cudaError_t result){
        if(result != cudaSuccess) throw std::runtime_error("Error: CUDA transport failure");
        return result;
    }

    class channel {
    public:
        struct mount {
            mount(){
                for(int k = 0; k < NSTREAMS; k++) checkCuda( cudaStreamCreate(&streams[k]) );
            }
           ~mount(){
                for(int k = 0; k < NSTREAMS; k++) checkCuda( cudaStreamDestroy(streams[k]) );
                cudaDeviceReset();
            }
            cudaStream_t streams[NSTREAMS];
        };
        static mount& setup(){ 
            static mount m; 
            return m; 
        }
        channel(){
	    channel::setup(); // making sure CUDA is initialised
        }
    };

} } }

#undef NSTREAMS
#endif
#endif
// }}}
// {{{ core package (requires :model :transport)

#ifndef BIND_CORE_COLLECTOR_H
#define BIND_CORE_COLLECTOR_H

namespace bind{ namespace memory {

    using model::history;
    using model::revision;
    using model::any;

    class collector {
    public:
        struct delete_ptr {
            void operator()( history* element ) const;
            void operator()( revision* element ) const;
            void operator()( any* element ) const;
        };

        void reserve(size_t n);
        void push_back(history* o);
        void push_back(revision* o);
        void push_back(any* o);
        void squeeze(revision* r) const;
        void clear();
    private:
        size_t reserve_limit;
        std::vector< history* >  str;
        std::vector< revision* > rev;
        std::vector< any* >      raw;
    };

} }

#endif


#ifndef BIND_CORE_COLLECTOR_HPP
#define BIND_CORE_COLLECTOR_HPP

namespace bind { namespace memory {

    using model::history;
    using model::revision;
    using model::any;

    inline void collector::reserve(size_t n){
        this->rev.reserve(n);
        this->str.reserve(n);
    }

    inline void collector::push_back(any* o){
        this->raw.push_back(o);
    }

    inline void collector::push_back(revision* r){
        r->weaken();
        if(!r->referenced()){ // squeeze
            if(!r->locked()) r->spec.free(r->data); // artifacts or last one
            this->rev.push_back(r);
        }
    }

    inline void collector::push_back(history* o){
        this->push_back(o->current);
        this->str.push_back(o);
    }

    inline void collector::squeeze(revision* r) const {
        if(!r->referenced() && r->locked_once()) r->spec.free(r->data);
    }

    inline void collector::delete_ptr::operator()(revision* r) const {
        r->spec.free(r->data);
        delete r; 
    }

    inline void collector::delete_ptr::operator()(history* e) const {
        delete e;
    }

    inline void collector::delete_ptr::operator()(any* e) const {
        memory::cpu::standard::free(e);
    } 

    inline void collector::clear(){
        std::for_each( rev.begin(), rev.end(), delete_ptr());
        std::for_each( str.begin(), str.end(), delete_ptr());
        std::for_each( raw.begin(), raw.end(), delete_ptr());
        rev.clear();
        str.clear();
        raw.clear();
    }

} }

#endif

#ifndef BIND_CORE_NODE_H
#define BIND_CORE_NODE_H

namespace bind {

    namespace core {
        class controller;
    }

    class node {
    protected:
        typedef core::controller controller_type;
        node(){}
    public:
       ~node();
        node(std::vector<rank_t>::const_iterator it);
        node(const rank_t r);
        bool remote() const;
        bool local()  const;
        bool common() const;
        rank_t which()  const;
        friend class core::controller;
    protected:
        rank_t rank;
        locality state;
        controller_type* controller;
    };

    struct node_each : public node {
        node_each(typename node::controller_type* c);
    };

}

#endif

#ifndef BIND_CORE_CONTROLLER_H
#define BIND_CORE_CONTROLLER_H

namespace bind { namespace core {

    using model::history;
    using model::revision;
    using model::any;
    using model::functor;

    class controller {
        controller(const controller&) = delete;
        controller& operator=(const controller&) = delete;
        controller(); 
    public:
        typedef transport::BIND_CHANNEL_NAME::channel channel_type;
       ~controller();
        void flush();
        void clear();
        bool queue (functor* f);
        bool update(revision& r);

        template<class Device, locality L, typename T> void sync(T* o);
        template<typename T> void collect(T* o);
        void squeeze(revision* r) const;

        void touch(const history* o, rank_t owner);
        void use_revision(history* o);
        template<locality L>
        void add_revision(history* o, functor* g, rank_t owner);

        bool verbose() const;
        rank_t get_rank() const;
        int get_num_procs() const;
        channel_type& get_channel();

        void sync();

        controller* activate(node* a);
        void deactivate(node* a);
        node& get_node();
    private:
        size_t clock;
        channel_type channel;
        std::vector< functor* > stack_m;
        std::vector< functor* > stack_s;
        std::vector< functor* >* chains;
        std::vector< functor* >* mirror;
        memory::collector garbage;
        utils::funneled_io io_guard;
        node_each* each;
        node* which;
    public:
        std::vector<rank_t> nodes;
        template<class T>
        struct weak_instance {
            static controller w;
        };
    };
    
} }

namespace bind {
    template<class T> core::controller core::controller::weak_instance<T>::w;
    inline core::controller& select(){ return core::controller::weak_instance<void>::w; }
}

#endif

#ifndef BIND_CORE_GET_H
#define BIND_CORE_GET_H

namespace bind { namespace core {
    
    template<class T> class get {};

    template<>
    class get<any> : public functor, public memory::cpu::use_bulk_new<get<any> > {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(any& v);
        get(any& v);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        collective<any>* handle;
        any& t;
    };

    template<>
    class get<revision> : public functor, public memory::cpu::use_bulk_new<get<revision> >  {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(revision& r);
        get(revision& r);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        void operator += (rank_t rank);
    private:
        collective<revision>* handle;
        revision& t;
    };

} }

#endif


#ifndef BIND_CORE_SET_H
#define BIND_CORE_SET_H

namespace bind { namespace core {

    template<class T> class set {};

    template<>
    class set<any> : public functor, public memory::cpu::use_bulk_new<set<any> > {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(any& v);
        set(any& v);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        collective<any>* handle;
        any& t;
    };

    template<>
    class set<revision> : public functor, public memory::cpu::use_bulk_new<set<revision> > {
    public:
        template<class T> using collective = controller::channel_type::collective_type<T>;
        static void spawn(revision& r);
        set(revision& r);
        virtual void invoke() override;
        virtual bool ready() override;
    private:
        void operator += (rank_t rank);
    private:
        collective<revision>* handle;
        revision& t;
    };

} }

#endif

#ifndef BIND_CORE_HUB_HPP
#define BIND_CORE_HUB_HPP

namespace bind { namespace nodes {
    inline size_t size(){
        return select().nodes.size();
    }
    inline std::vector<rank_t>::const_iterator begin(){
        return select().nodes.begin();
    }
    inline std::vector<rank_t>::const_iterator end(){
        return select().nodes.end();
    }
    inline rank_t which_(){
        return select().get_node().which();
    }
    template<typename V>
    inline rank_t which(const V& o){
        return o.allocator_.desc->current->owner;
    }
    inline rank_t which(){
        rank_t w = which_();
        return (w == select().get_num_procs() ? select().get_rank() : w);
    }
} }


namespace bind { namespace transport {

    using model::revision;
    using model::any;

    template<class Device, locality L = locality::common>
    struct hub {
        static void sync(revision* r){
            if(bind::nodes::size() == 1) return; // serial
            if(model::common(r)) return;
            if(model::local(r)) core::set<revision>::spawn(*r);
            else core::get<revision>::spawn(*r);
        }
    };

    template<class Device>
    struct hub<Device, locality::local> {
        static void sync(revision* r){
            if(model::common(r)) return;
            if(!model::local(r)) core::get<revision>::spawn(*r);
        }
        static void sync(any* v){
            if(bind::nodes::size() == 1) return;
            core::set<any>::spawn(*v);
        }
    };

    template<class Device>
    struct hub<Device, locality::remote> {
        static void sync(revision* r){
            if(r->owner == bind::nodes::which_() || model::common(r)) return;
            if(model::local(r)) core::set<revision>::spawn(*r);
            else core::get<revision>::spawn(*r); // assist
        }
        static void sync(any* v){
            core::get<any>::spawn(*v);
        }
    };

} }

#endif

#ifndef BIND_CORE_CONTROLLER_HPP
#define BIND_CORE_CONTROLLER_HPP

namespace bind { namespace core {

    inline controller::~controller(){ 
        if(!chains->empty()) printf("Bind:: exiting with operations still in queue!\n");
        this->clear();
        delete this->each;
    }

    inline controller::controller() : chains(&stack_m), mirror(&stack_s), clock(1) {
        this->each = new node_each(this);
        this->which = NULL;
        for(int i = 0; i < get_num_procs(); i++) nodes.push_back(i);
        if(!verbose()) this->io_guard.enable();
    }

    inline void controller::deactivate(node* a){
        which = NULL;
    }

    inline controller* controller::activate(node* n){
        if(which) return NULL;
        which = n;
        return this;
    }

    inline void controller::sync(){
        this->flush();
        this->clear();
        memory::cpu::instr_bulk::drop();
        memory::cpu::data_bulk::drop();
        memory::cpu::comm_bulk::drop();
    }

    inline node& controller::get_node(){
        return (!which) ? *each : *which;
    }

    inline void controller::flush(){
        while(!chains->empty()){
            for(auto task : *chains){
                if(task->ready()){
                    cilk_spawn task->invoke();
                    for(auto d : task->deps) d->ready();
                    mirror->insert(mirror->end(), task->deps.begin(), task->deps.end());
                }else mirror->push_back(task);
            }
            chains->clear();
            std::swap(chains,mirror);
        }
        cilk_sync;
        clock++;
        channel.barrier();
    }

    inline void controller::clear(){
        this->garbage.clear();
    }

    inline bool controller::queue(functor* f){
        this->chains->push_back(f);
        return true;
    }

    inline bool controller::update(revision& r){
        if(r.assist.first != clock){
            r.assist.first = clock;
            return true;
        }
        return false;
    }

    template<class Device, locality L, typename T>
    inline void controller::sync(T* o){
        transport::hub<Device, L>::sync(o);
    }

    template<typename T> void controller::collect(T* o){
        this->garbage.push_back(o);
    }

    inline void controller::squeeze(revision* r) const {
        this->garbage.squeeze(r);
    }

    inline void controller::touch(const history* o, rank_t owner){
        if(o->back() == NULL)
            const_cast<history*>(o)->init_state(owner);
    }

    inline void controller::use_revision(history* o){
        o->back()->use();
    }

    template<locality L>
    void controller::add_revision(history* o, functor* g, rank_t owner){
        o->add_state<L>(g, owner);
    }

    inline rank_t controller::get_rank() const {
        return channel.rank;
    }

    inline bool controller::verbose() const {
        return (get_rank() == 0);
    }

    inline int controller::get_num_procs() const {
        return channel.dim();
    }

    inline typename controller::channel_type & controller::get_channel(){
        return channel;
    }

} }

#endif

#ifndef BIND_CORE_GET_HPP
#define BIND_CORE_GET_HPP

namespace bind { namespace core {

    // {{{ any

    inline void get<any>::spawn(any& t){
        bind::select().queue(new get(t));
    }
    inline get<any>::get(any& ptr) : t(ptr) {
        handle = bind::select().get_channel().bcast(t, bind::nodes::which_());
        t.generator = this;
    }
    inline bool get<any>::ready(){
        return handle->test();
    }
    inline void get<any>::invoke(){
        t.complete();
    }

    // }}}
    // {{{ revision

    inline void get<revision>::spawn(revision& r){
        get*& transfer = (get*&)r.assist.second;
        if(bind::select().update(r)) transfer = new get(r);
        *transfer += bind::nodes::which_();
    }
    inline get<revision>::get(revision& r) : t(r) {
        handle = bind::select().get_channel().get(t);
        t.invalidate();
    }
    inline void get<revision>::operator += (rank_t rank){
        handle->append(rank);
        if(handle->involved() && !t.valid()){
            t.use();
            t.generator = this;
            t.embed(t.spec.hard_malloc<memory::cpu::comm_bulk>()); 
            bind::select().queue(this);
        }
    }
    inline bool get<revision>::ready(){
        return handle->test();
    }
    inline void get<revision>::invoke(){
        bind::select().squeeze(&t);
        t.release();
        t.complete();
    }

    // }}}

} }

#endif

#ifndef BIND_CORE_SET_HPP
#define BIND_CORE_SET_HPP

namespace bind { namespace core {

    // {{{ any

    inline void set<any>::spawn(any& t){
        (t.generator.load())->queue(new set(t));
    }
    inline set<any>::set(any& t) : t(t) {
        handle = bind::select().get_channel().bcast(t, bind::nodes::which_());
    }
    inline bool set<any>::ready(){
        return (t.generator != NULL ? false : handle->test());
    }
    inline void set<any>::invoke(){}

    // }}}
    // {{{ revision

    inline void set<revision>::spawn(revision& r){
        set*& transfer = (set*&)r.assist.second;
        if(bind::select().update(r)) transfer = new set(r);
        *transfer += bind::nodes::which_();
    }
    inline set<revision>::set(revision& r) : t(r) {
        t.use();
        handle = bind::select().get_channel().set(t);
        if(t.generator != NULL) (t.generator.load())->queue(this);
        else bind::select().queue(this);
    }
    inline void set<revision>::operator += (rank_t rank){
        handle->append(rank);
    }
    inline bool set<revision>::ready(){
        return (t.generator != NULL ? false : handle->test());
    }
    inline void set<revision>::invoke(){
        bind::select().squeeze(&t);
        t.release(); 
    }

    // }}}

} }

#endif

#ifndef BIND_CORE_NODE_HPP
#define BIND_CORE_NODE_HPP

namespace bind {

    // {{{ primary node-class

    inline node::~node(){
        if(!controller) return;
        bind::select().deactivate(this);
    }
    inline node::node(const rank_t r){
        if(! (controller = bind::select().activate(this)) ) return;
        this->state = (r == controller->get_rank()) ? locality::local : locality::remote;
        this->rank = r;
    }
    inline node::node(std::vector<rank_t>::const_iterator it) : node(*it)
    {
    }
    inline bool node::remote() const {
        return (state == locality::remote);
    }
    inline bool node::local() const {
        return (state == locality::local);
    }
    inline bool node::common() const {
        return (state == locality::common);
    }
    inline rank_t node::which() const {
        return this->rank;
    }

    // }}}
    // {{{ node's special case: everyone does the same

    inline node_each::node_each(typename node::controller_type* c){
        this->controller = c;
        this->rank = controller->get_num_procs();
        this->state = locality::common;
    }

    // }}}
}

#endif
// }}}
// {{{ interface package (requires :model :transport :core)

#ifndef BIND_INTERFACE_DEVICES
#define BIND_INTERFACE_DEVICES

namespace bind { namespace devices {

    class cpu;
    class gpu;

} }

#endif


#ifndef BIND_INTERFACE_SHORTCUTS
#define BIND_INTERFACE_SHORTCUTS

namespace bind {

    inline void sync(){
        bind::select().sync();
    }

    inline int num_procs(){
        return bind::select().get_num_procs();
    }

    inline int num_threads(){
        static int n = __cilkrts_get_nworkers(); return n;
    }

    inline rank_t rank(){
        return bind::select().get_rank();
    }

    template<typename T>
    inline void destroy(T* o){ 
        bind::select().collect(o); 
    }

    template<typename V>
    inline bool weak(const V& obj){
        return obj.allocator_.desc->weak();
    }

    template<typename V>
    inline size_t extent(V& obj){ 
        return obj.allocator_.desc->extent;
    }

}

#endif

#ifndef BIND_INTERFACE_MODIFIERS_SINGULAR
#define BIND_INTERFACE_MODIFIERS_SINGULAR

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;

    template <typename T, bool Compact = false>
    struct singular_modifier {
        template<size_t Arg> static void deallocate(functor* ){ }
        template<size_t Arg> static bool pin(functor* ){ return false; }
        template<size_t Arg> static bool ready(functor* ){ return true; }
        template<size_t Arg> static void load(functor* m){ }
        template<size_t Arg> static T&   forward(functor* m){ EXTRACT(o); return o; }
        template<size_t Arg> static void apply_remote(T&){ }
        template<size_t Arg> static void apply_local(T& o, functor* m){
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T));
        }
        template<size_t Arg> static void apply_common(T& o, functor* m){
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T));
        }
        static constexpr bool ReferenceOnly = false;
    };

    template <typename T>
    struct singular_modifier<T, true> : public singular_modifier<T> {
        template<size_t Arg> static T& forward(functor* m){ return *(T*)&m->arguments[Arg]; }
        template<size_t Arg> static void apply_local(T& o, functor* m){ *(T*)&m->arguments[Arg] = o; }
        template<size_t Arg> static void apply_common(T& o, functor* m){ *(T*)&m->arguments[Arg] = o; }
    };
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_SHARED_PTR
#define BIND_INTERFACE_MODIFIERS_SHARED_PTR

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;

    template <typename T>
    struct const_shared_ptr_modifier : public singular_modifier<T> {
        template<size_t Arg> static bool ready(functor* m){
            EXTRACT(o);
            if(o.impl->origin && o.impl->origin->generator != NULL) return false;
            return (o.impl->generator == m || o.impl->generator == NULL);
        }
        template<size_t Arg> static bool pin(functor* m){
            EXTRACT(o);
            if(o.impl->generator == NULL) return false;
            (o.impl->generator.load())->queue(m);
            return true;
        }
        static constexpr bool ReferenceOnly = true;
    };

    template <typename T>
    struct shared_ptr_modifier : public const_shared_ptr_modifier<T> {
        template<size_t Arg> static void deallocate(functor* m){
            EXTRACT(o); o.impl->complete();
        }
        template<size_t Arg> static bool pin(functor* m){
            EXTRACT(o);
            if(!o.impl->origin || o.impl->origin->generator == NULL) return false;
            (o.impl->origin->generator.load())->queue(m);
            return true;
        }
        template<size_t Arg> static void apply_remote(T& o){
            o.resit();
            bind::select().sync<devices::cpu, locality::remote>(o.impl);
        }
        template<size_t Arg> static void apply_local(T& o, functor* m){
            if(o.impl->generator != m){
                o.resit();
                o.impl->generator = m;
            }
            bind::select().sync<devices::cpu, locality::local>(o.impl);
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T)); 
        }
        template<size_t Arg> static void apply_common(T& o, functor* m){
            if(o.impl->generator != m){
                o.resit();
                o.impl->generator = m;
            }
            m->arguments[Arg] = memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy(m->arguments[Arg], &o, sizeof(T)); 
        }
        template<size_t Arg> static void load(functor* m){
            EXTRACT(o);
            if(o.impl->origin){
                *o.impl = (typename T::element_type&)*o.impl->origin;
                o.impl->origin = NULL;
            }
        }
    };

    template <typename T>
    struct volatile_shared_ptr_modifier : public shared_ptr_modifier<T> {
        template<size_t Arg> static void apply_remote(T& o){ }
        template<size_t Arg> static void apply_local(T& o, functor* m){
            apply_common<Arg>(o, m);
        }
        template<size_t Arg> static void apply_common(T& o, functor* m){
            shared_ptr_modifier<typename std::remove_volatile<T>::type>::apply_common<Arg>(const_cast<typename std::remove_volatile<T>::type&>(o), m);
        }
    };
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_ITERATOR
#define BIND_INTERFACE_MODIFIERS_ITERATOR

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;
    template <class Device, typename T> struct modifier;

    template <class Device, typename T>
    struct iterator_modifier : public singular_modifier<T> {
        typedef typename modifier<Device, typename T::container_type>::type type;
        typedef typename T::container_type container_type;

        template<size_t Arg> 
        static void deallocate(functor* m){
            EXTRACT(o); type::deallocate_(*o.container);
        }
        template<size_t Arg>
        static void apply_remote(T& o){
            type::template apply_remote<Arg>(*o.container);
        }
        template<size_t Arg>
        static void apply_local(T& o, functor* m){
            type::template apply_local<Arg>(*o.container, m);
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &o, sizeof(T));
            var->container = (container_type*)m->arguments[Arg]; m->arguments[Arg] = (void*)var;
        }
        template<size_t Arg>
        static void apply_common(T& o, functor* m){
            type::template apply_common<Arg>(*o.container, m);
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &o, sizeof(T));
            var->container = (container_type*)m->arguments[Arg]; m->arguments[Arg] = (void*)var;
        }
        template<size_t Arg>
        static void load(functor* m){
            EXTRACT(o); type::load_(*o.container);
        }
        template<size_t Arg> 
        static bool pin(functor* m){ 
            EXTRACT(o); return type::pin_(*o.container, m);
        }
        template<size_t Arg> 
        static bool ready(functor* m){
            EXTRACT(o); return type::ready_(*o.container, m);
        }
    };
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_VERSIONED
#define BIND_INTERFACE_MODIFIERS_VERSIONED

#define EXTRACT(var) T& var = *(T*)m->arguments[Arg];

namespace bind {
    using model::functor;
    using model::revision;

    template <class Device, typename T>
    struct versioned_modifier : public singular_modifier<T> {
        template<size_t Arg> 
        static void deallocate(functor* m){
            EXTRACT(o); deallocate_(o);
        }
        template<size_t Arg> 
        static bool pin(functor* m){
            EXTRACT(o); return pin_(o,m);
        }
        template<size_t Arg> 
        static bool ready(functor* m){
            EXTRACT(o); return ready_(o, m);
        }
        template<size_t Arg>
        static void load(functor* m){ 
            EXTRACT(o); load_(o);
        }
        static void deallocate_(T& o){
            revision& parent  = *o.allocator_.before;
            revision& current = *o.allocator_.after;
            current.complete();
            current.release();
            bind::select().squeeze(&parent);
            parent.release();
        }
        template<locality L, size_t Arg>
        static void apply_(T& obj, functor* m){
            auto o = obj.allocator_.desc;
            bind::select().touch(o, bind::rank());
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &obj, sizeof(T)); 
            m->arguments[Arg] = (void*)var;
            bind::select().sync<Device, L>(o->current);
            bind::select().use_revision(o);

            var->allocator_.before = o->current;
            if(o->current->generator != m){
                bind::select().collect(o->current);
                bind::select().add_revision<L>(o, m, bind::rank());
            }
            bind::select().use_revision(o);
            var->allocator_.after = obj.allocator_.after = o->current;
        }
        template<size_t Arg>
        static void apply_remote(T& obj){
            auto o = obj.allocator_.desc;
            bind::select().touch(o, bind::rank());
            bind::select().sync<Device, locality::remote>(o->current);
            bind::select().collect(o->current);
            bind::select().add_revision<locality::remote>(o, NULL, bind::nodes::which_()); 
        }
        template<size_t Arg>
        static void apply_local(T& obj, functor* m){
            apply_<locality::local, Arg>(obj, m);
        }
        template<size_t Arg>
        static void apply_common(T& obj, functor* m){
            apply_<locality::common, Arg>(obj, m);
        }
        static bool pin_(T& o, functor* m){
            revision& r = *o.allocator_.before;
            if(r.generator != NULL && r.generator != m){
                (r.generator.load())->queue(m);
                return true;
            }
            return false;
        }
        static bool ready_(T& o, functor* m){
            revision& r = *o.allocator_.before;
            if(r.generator == NULL || r.generator == m) return true;
            return false;
        }
        static void load_(T& o){ 
            revision& c = *o.allocator_.after; if(c.valid()) return;
            revision& p = *o.allocator_.before;
            if(!p.valid()){
                c.embed(c.spec.calloc<Device>());
            }else if(!p.locked_once() || p.referenced() || !c.spec.conserves<Device>(p.spec)){
                c.embed(c.spec.malloc<Device>());
                c.spec.memcpy<Device>(c.data, p.data, p.spec);
            }else
                c.reuse(p);
        }
        static constexpr bool ReferenceOnly = true;
    };
    // {{{ compile-time type modifier: const/volatile cases of the versioned types
    template <class Device, typename T>
    struct const_versioned_modifier : public versioned_modifier<Device, T> {
        template<size_t Arg>
        static void deallocate(functor* m){
            EXTRACT(o); deallocate_(o);
        }
        static void deallocate_(T& o){
            revision& r = *o.allocator_.before;
            bind::select().squeeze(&r);
            r.release();
        }
        template<size_t Arg>
        static void load(functor* m){ 
            EXTRACT(o); load_(o);
        }
        static void load_(T& o){
            revision& c = *o.allocator_.before;
            if(!c.valid()) c.embed(c.spec.calloc<Device>());
        }
        template<locality L, size_t Arg> static void apply_(T& obj, functor* m){
            auto o = obj.allocator_.desc;
            bind::select().touch(o, bind::rank());
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, &obj, sizeof(T)); m->arguments[Arg] = (void*)var;
            bind::select().sync<Device, L>(o->current);
            bind::select().use_revision(o);
            var->allocator_.before = var->allocator_.after = o->current;
        }
        template<size_t Arg> static void apply_remote(T& obj){
            auto o = obj.allocator_.desc;
            bind::select().touch(o, bind::rank());
            bind::select().sync<Device, locality::remote>(o->current);
        }
        template<size_t Arg> static void apply_local(T& obj, functor* m){
            apply_<locality::local, Arg>(obj, m);
        }
        template<size_t Arg> static void apply_common(T& obj, functor* m){
            apply_<locality::common, Arg>(obj, m);
        }
    };
    template <class Device, typename T>
    struct volatile_versioned_modifier : public versioned_modifier<Device, T> {
        template<size_t Arg> static void load(functor* m){ 
            EXTRACT(o); load_(o);
        }
        static void load_(T& o){
            revision& c = *o.allocator_.after; if(c.valid()) return; // can it occur?
            revision& p = *o.allocator_.before;
            if(!p.valid() || !p.locked_once() || p.referenced() || !c.spec.conserves<Device>(p.spec)){
                c.embed(c.spec.malloc<Device>());
            }else
                c.reuse(p);
        }
        template<locality L, size_t Arg> static void apply_(T& obj, functor* m){
            auto o = obj.allocator_.desc;
            bind::select().touch(o, bind::rank());
            T* var = (T*)memory::cpu::instr_bulk::malloc<sizeof(T)>(); memcpy((void*)var, (void*)&obj, sizeof(T)); m->arguments[Arg] = (void*)var;
            bind::select().use_revision(o);

            var->allocator_.before = o->current;
            if(o->current->generator != m){
                bind::select().collect(o->current);
                bind::select().add_revision<L>(o, m, bind::rank()); 
            }
            bind::select().use_revision(o);
            var->allocator_.after = obj.allocator_.after = o->current;
        }
        template<size_t Arg> static void apply_remote(T& obj){
            auto o = obj.allocator_.desc;
            bind::select().touch(o, bind::rank());
            bind::select().collect(o->current);
            bind::select().add_revision<locality::remote>(o, NULL, bind::nodes::which_()); 
        }
        template<size_t Arg> static void apply_local(T& obj, functor* m){
            apply_<locality::local, Arg>(obj, m);
        }
        template<size_t Arg> static void apply_common(T& obj, functor* m){
            apply_<locality::common, Arg>(obj, m);
        }
        template<size_t Arg> static bool pin(functor* m){ return false; }
        template<size_t Arg> static bool ready(functor* m){ return true; }
        static bool pin_(T&, functor*){ return false; }
        static bool ready_(T&, functor*){ return true; }
    };
    // }}}
}

#undef EXTRACT
#endif

#ifndef BIND_INTERFACE_MODIFIERS_DISPATCH
#define BIND_INTERFACE_MODIFIERS_DISPATCH

namespace bind {
    namespace detail {
        template<typename T>
        constexpr bool compact(){ return sizeof(T) <= sizeof(void*); }

        template <typename T> struct has_versioning {
            template <typename T1> static typename T1::allocator_type::bind_type test(int);
            template <typename> static void test(...);
            enum { value = !std::is_void<decltype(test<T>(0))>::value };
        };
        template <bool Versioned, class Device, typename T> struct get_modifier { typedef singular_modifier<T, compact<T>()> type; };
        template<class Device, typename T> struct get_modifier<true, Device, T> { typedef versioned_modifier<Device, T> type; };

        template <bool Versioned, class Device, typename T> struct const_get_modifier { typedef singular_modifier<const T, compact<T>()> type; };
        template<class Device, typename T> struct const_get_modifier<true, Device, T> { typedef const_versioned_modifier<Device, const T> type; };

        template <bool Versioned, class Device, typename T> struct volatile_get_modifier { typedef singular_modifier<volatile T, compact<T>()> type; };
        template<class Device, typename T> struct volatile_get_modifier<true, Device, T> { typedef volatile_versioned_modifier<Device, volatile T> type; };
    }

    template <typename T> class proxy_iterator;
    template <typename T> class shared_ptr;

    template <class Device, typename T> struct modifier {
        typedef typename detail::get_modifier<detail::has_versioning<T>::value, Device, T>::type type;
    };
    template <class Device, typename T> struct modifier<Device, const T> {
        typedef typename detail::const_get_modifier<detail::has_versioning<T>::value, Device, T>::type type;
    };
    template <class Device, typename T> struct modifier<Device, volatile T> {
        typedef typename detail::volatile_get_modifier<detail::has_versioning<T>::value, Device, T>::type type;
    };
    template <class Device, typename S> struct modifier<Device, shared_ptr<S> > {
        typedef shared_ptr_modifier< shared_ptr<S> > type; 
    };
    template <class Device, typename S> struct modifier<Device, const shared_ptr<S> > {
        typedef const_shared_ptr_modifier< const shared_ptr<S> > type; 
    };
    template <class Device, typename S> struct modifier<Device, volatile shared_ptr<S> > {
        typedef volatile_shared_ptr_modifier< volatile shared_ptr<S> > type; 
    };
    template <class Device, typename S> struct modifier<Device, proxy_iterator<S> > {
        typedef iterator_modifier<Device, proxy_iterator<S> > type;
    };
}

#endif

#ifndef BIND_INTERFACE_SNAPSHOT
#define BIND_INTERFACE_SNAPSHOT

namespace bind {

    using model::revision;

    struct snapshot {
        typedef model::history bind_type;

        snapshot& operator=(const snapshot&) = delete;
        snapshot(){ }
        snapshot(size_t size){
            desc = new bind_type(size);
        }
        snapshot(const snapshot& origin){
            desc = new bind_type(origin.desc->extent);
            revision* r = origin.desc->back(); if(!r) return;
            desc->current = r;
            r->protect();
        }
       ~snapshot(){
            if(desc->weak()) delete desc;
            else destroy(desc);
        }
        void* data() volatile {
            return after->data;
        }
        revision* before = NULL;
        revision* after = NULL;
        bind_type* desc;
    };
}

#endif

#ifndef BIND_INTERFACE_KERNEL_INLINER
#define BIND_INTERFACE_KERNEL_INLINER

namespace bind {
    using model::functor;

    template<class Device, typename T>
    struct check_if_not_reference {
        template<bool C, typename F>  struct fail_if_true { typedef F type; };
        template<typename F> struct fail_if_true<true, F> { };
        typedef typename fail_if_true<modifier<Device, T>::type::ReferenceOnly, T>::type type; // T can be passed only by reference
    };
 
    template<class Device, typename T>
    struct check_if_not_reference<Device, T&> {
        typedef T type;
    };

    template <class Device, typename T>
    using checked_remove_reference = typename std::remove_reference<
                                         typename check_if_not_reference< Device, T >::type 
                                     >::type;

    template<class Device, int N> void expand_modify_remote(){}
    template<class Device, int N> void expand_modify_local(functor* o){}
    template<class Device, int N> void expand_modify_common(functor* o){}
    template<class Device, int N> bool expand_pin(functor* o){ return false; }
    template<class Device, int N> void expand_load(functor* o){ }
    template<class Device, int N> void expand_deallocate(functor* o){ }
    template<class Device, int N> bool expand_ready(functor* o){ return true; }

    template<class Device, int N, typename T, typename... TF>
    void expand_modify_remote(T& arg, TF&... other){
        modifier<Device, T>::type::template apply_remote<N>(arg);
        expand_modify_remote<Device, N+1>(other...);
    }
    template<class Device, int N, typename T, typename... TF>
    void expand_modify_local(functor* o, T& arg, TF&... other){
        modifier<Device, T>::type::template apply_local<N>(arg, o);
        expand_modify_local<Device, N+1>(o, other...);
    }
    template<class Device, int N, typename T, typename... TF>
    void expand_modify_common(functor* o, T& arg, TF&... other){
        modifier<Device, T>::type::template apply_common<N>(arg, o);
        expand_modify_common<Device, N+1>(o, other...);
    }
    template<class Device, int N, typename T, typename... TF>
    bool expand_pin(functor* o){
        return modifier<Device, checked_remove_reference<Device, T> >::type::template pin<N>(o) ||
               expand_pin<Device, N+1, TF...>(o);
    }
    template<class Device, int N, typename T, typename... TF>
    void expand_load(functor* o){
        modifier<Device, checked_remove_reference<Device, T> >::type::template load<N>(o);
        expand_load<Device, N+1, TF...>(o);
    }
    template<class Device, int N, typename T, typename... TF>
    void expand_deallocate(functor* o){
        modifier<Device, checked_remove_reference<Device, T> >::type::template deallocate<N>(o);
        expand_deallocate<Device, N+1, TF...>(o);
    }
    template<class Device, int N, typename T, typename... TF>
    bool expand_ready(functor* o){
        return modifier<Device, checked_remove_reference<Device, T> >::type::template ready<N>(o) &&
               expand_ready<Device, N+1, TF...>(o);
    }

    template<class Device, typename FP, FP fp>
    struct kernel_inliner {};

    template<class Device, typename... TF , void(*fp)( TF... )>
    struct kernel_inliner<Device, void(*)( TF... ), fp> {
        static const int arity = sizeof...(TF);

        static inline void latch(functor* o, TF&... args){
            #ifndef BIND_TRANSPORT_NOP
            if(bind::select().get_node().remote())   { expand_modify_remote<Device, 0>(args...); return; }
            else if(bind::select().get_node().local()) expand_modify_local<Device, 0>(o, args...);
            else
            #endif
            expand_modify_common<Device, 0>(o, args...);
            expand_pin<Device, 0, TF...>(o) || bind::select().queue(o);
        }
        static inline void cleanup(functor* o){
            expand_deallocate<Device, 0, TF...>(o);
        }
        static inline bool ready(functor* o){
            return expand_ready<Device, 0, TF...>(o);
        }
        template<size_t...I>
        static void expand_invoke(index_sequence<I...>, functor* o){
            (*fp)(modifier<Device, checked_remove_reference<Device, TF> >::type::template forward<I>(o)...);
        }
        static inline void invoke(functor* o){
            expand_load<Device, 0, TF...>(o);
            expand_invoke(make_index_sequence<sizeof...(TF)>(), o);
        }
    };

}

#endif


#ifndef BIND_INTERFACE_KERNEL
#define BIND_INTERFACE_KERNEL

namespace bind {

    using model::functor;

    template<typename Device, class K>
    class kernel : public functor {
    public:
        #define inliner kernel_inliner<Device, typename K::ftype, K::c>
        inline void operator delete (void* ptr){ }
        inline void* operator new (size_t size){
            return memory::cpu::instr_bulk::malloc<sizeof(K)+sizeof(void*)*inliner::arity>();
        }
        virtual bool ready() override { 
            return inliner::ready(this);
        }
        virtual void invoke() override {
            inliner::invoke(this);
            inliner::cleanup(this);
        }
        template<size_t...I, typename... Args>
        static void expand_spawn(index_sequence<I...>, Args&... args){
            inliner::latch(new kernel(), args...);
        }
        template<typename... Args>
        static inline void spawn(Args&& ... args){
            expand_spawn(make_index_sequence<sizeof...(Args)>(), args...);
        }
        #undef inliner
    };
}

#endif

#ifndef BIND_INTERFACE_LAMBDA
#define BIND_INTERFACE_LAMBDA

namespace bind {

    template<class Device, typename F, typename... T>
    struct lambda_kernel : public kernel<Device, lambda_kernel<Device, F, T...> > {
        typedef void(*ftype)(T..., F&);
        static void fw(T... args, F& func){ func(args...); }
        static constexpr ftype c = &fw;
    };

    template <typename Function>
    struct function_traits : public function_traits<decltype(&Function::operator())> {};

    template <typename ClassType, typename ReturnType, typename... Args>
    struct function_traits<ReturnType(ClassType::*)(Args...) const> {
        typedef ReturnType (*pointer)(Args...);
        typedef const std::function<ReturnType(Args...)> function;
        template<class Device>
        using kernel_type = lambda_kernel<Device, const std::function<ReturnType(Args...)>, Args... >;
    };

    template <typename Function>
    typename function_traits<Function>::function to_function(Function& lambda){
        return static_cast<typename function_traits<Function>::function>(lambda);
    }

    template <class Device, class L>
    struct overload_lambda : L {
        overload_lambda(L l) : L(l) {}
        template <typename... T>
        void operator()(T&& ... values){
            function_traits<L>::template kernel_type<Device>::spawn(std::forward<T>(values)... , to_function(*(L*)this));
        }
    };

    template <class Device, class L>
    overload_lambda<Device, L> lambda(L l){
        return overload_lambda<Device, L>(l);
    }

    template <class L, class... Args>
    void cpu(L l, Args&& ... args){
        lambda<devices::cpu>(l)(std::forward<Args>(args)...);
    }

    template <class... L, class R, class... Args>
    void cpu(R(*l)(L...), Args&& ... args){
        bind::cpu(std::function<R(L...)>(l), std::forward<Args>(args)...);
    }

    template <class L, class... Args>
    void gpu(L l, Args&& ... args){
        lambda<devices::gpu>(l)(std::forward<Args>(args)...);
    }

    template <class... L, class R, class... Args>
    void gpu(R(*l)(L...), Args&& ... args){
        bind::gpu(std::function<R(L...)>(l), std::forward<Args>(args)...);
    }
}

#endif

// }}}
// {{{ container package (requires :*)

#ifndef BIND_CONTAINER_SMART_PTR
#define BIND_CONTAINER_SMART_PTR

namespace bind {

    using model::any;
    using model::sizeof_any;

    template <typename T>
    class smart_ptr {
    protected:
        typedef T element_type;
       ~smart_ptr(){
           if(impl) bind::destroy(impl); 
        }
        smart_ptr(element_type val){
            impl = new (memory::cpu::standard::calloc<sizeof_any<T>()>()) any(val);
        }
        smart_ptr(const smart_ptr& f){
            impl = new (memory::cpu::standard::calloc<sizeof_any<T>()>()) any((element_type&)*f);
            impl->origin = f.impl;
        }
        smart_ptr(smart_ptr&& f){
            impl = f.impl; f.impl = NULL; 
        }
        smart_ptr& operator= (const smart_ptr& f){
            *impl = (element_type&)*f;
            impl->origin = f.impl;
            return *this;
        }
        smart_ptr& operator= (smart_ptr&& f){ 
            std::swap(impl, f.impl);
            return *this;
        }
        template<typename S>
        smart_ptr& operator= (const S& val) = delete;
    public:
        T& operator* () const volatile {
            return *impl;
        }
        void resit() const {
            smart_ptr clone(*this);
            std::swap(this->impl, clone.impl);
            this->impl->origin = clone.impl;
        }
        mutable any* impl;
    };

    template<typename T>
    class shared_ptr : public smart_ptr<T> {
    public:
        typedef typename smart_ptr<T>::element_type element_type;
        template <typename U>
        shared_ptr(U&& arg) : smart_ptr<T>(std::forward<U>(arg)) {}
    };

    template<class T>
    std::ostream& operator << (std::ostream& os, const smart_ptr<T>& obj){
        os << *obj;
        return os;
    }
}

#endif

#ifndef BIND_CONTAINER_PROXY_ITERATOR
#define BIND_CONTAINER_PROXY_ITERATOR

namespace bind {

    template<class Container>
    class proxy_iterator {
    public:
        typedef Container container_type;
        typedef typename std::iterator_traits<bind::proxy_iterator<Container> >::value_type value_type;

        proxy_iterator() : container(NULL), position(0) {}
        proxy_iterator(container_type& owner, size_t p) : container(&owner), position(p) {}
        proxy_iterator& operator += (size_t offset){
            position += offset;
            return *this;
        }
        proxy_iterator& operator++ (){
            position++;
            return *this;
        }
        proxy_iterator operator++ (int){
            proxy_iterator tmp(*this);
            operator++();
            return tmp;
        }
        proxy_iterator& operator -= (size_t offset){
            position -= offset;
            return *this;
        }
        proxy_iterator& operator-- (){
            position--;
            return *this;
        }
        proxy_iterator operator-- (int){
            proxy_iterator tmp(*this);
            operator--();
            return tmp;
        }
        value_type& operator* () const {
            return (*container)[position];
        }
        container_type& get_container(){
            return *container;
        }
        const container_type& get_container() const {
            return *container;
        }
        size_t position;
    public:
        template<typename T>
        friend bool operator == (const proxy_iterator<T>& lhs, const proxy_iterator<T>& rhs);
        template<typename T>
        friend bool operator != (const proxy_iterator<T>& lhs, const proxy_iterator<T>& rhs);
        container_type* container;
    };

    template <class Container> 
    bool operator == (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position == rhs.position && lhs.container->allocator_.desc == rhs.container->allocator_.desc);
    }

    template <class Container> 
    bool operator != (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position != rhs.position || lhs.container->allocator_.desc != rhs.container->allocator_.desc);
    }

    template <class Container, class OtherContainer> 
    size_t operator - (const proxy_iterator<Container>& lhs, const proxy_iterator<OtherContainer>& rhs){ 
        return lhs.position - rhs.position;
    }

    template <class Container> 
    proxy_iterator<Container> operator + (proxy_iterator<Container> lhs, size_t offset){ 
        return (lhs += offset);
    }

    template <class Container> 
    proxy_iterator<Container> operator - (proxy_iterator<Container> lhs, size_t offset){ 
        return (lhs -= offset);
    }

    template <class Container> 
    bool operator < (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position < rhs.position);
    }

    template <class Container> 
    bool operator > (const proxy_iterator<Container>& lhs, const proxy_iterator<Container>& rhs){
        return (lhs.position > rhs.position);
    }

}

namespace std {

    template<class Container>
    class iterator_traits<bind::proxy_iterator<Container> > {
    public:
        typedef std::random_access_iterator_tag iterator_category;
        typedef typename Container::value_type value_type;
        typedef size_t difference_type;
    };

    template<class Container>
    class iterator_traits<bind::proxy_iterator<const Container> > {
    public:
        typedef std::random_access_iterator_tag iterator_category;
        typedef const typename Container::value_type value_type;
        typedef size_t difference_type;
    };
}

#endif

#ifndef BIND_CONTAINER_ARRAY
#define BIND_CONTAINER_ARRAY

namespace bind {

    // {{{ array helper functions
    template<class T, class Allocator> class array;
    namespace detail {
        template<class T, class Allocator>
        void fill_array(volatile bind::array<T,Allocator>& a, T& value){
            for(size_t i = 0; i < a.size(); ++i) a[i] = value;
        }
        template<class T, class Allocator, class OtherAllocator = Allocator>
        void copy_array(volatile bind::array<T,Allocator>& dst, const bind::array<T,OtherAllocator>& src, const size_t& n){
            for(size_t i = 0; i < n; ++i) dst[i] = src[i];
        }
    }
    // }}}

    template <class T, class Allocator = bind::snapshot>
    class array {
    public:
        void* operator new (size_t size, void* ptr){ return ptr; }
        void  operator delete (void*, void*){ /* doesn't throw */ }
        void* operator new (size_t sz){ return memory::cpu::standard::malloc<sizeof(array)>(); }
        void operator delete (void* ptr){ memory::cpu::standard::free(ptr); }
    public:
        using allocator_type = Allocator;
        using value_type = T;
        using size_type = size_t;
        using volatile_iterator = proxy_iterator<volatile array>;
        using const_iterator = proxy_iterator<const array>;
        using iterator = proxy_iterator<array>;
        explicit array(){}
        array(const array& a) = default;
        explicit array(size_t n) : allocator_(n*sizeof(T)), size_(n) {}

        array& operator = (const array& rhs){
            array c(rhs);
            this->swap(c);
            return *this;
        }
        template<class OtherAllocator>
        array& operator = (const array<T,OtherAllocator>& rhs){
            array resized(rhs.size());
            this->swap(resized);
            if(!bind::weak(rhs)) bind::cpu(detail::copy_array<T,Allocator,OtherAllocator>, *this, rhs, this->size_);
            return *this;
        }
        void fill(T value){
            bind::cpu(detail::fill_array<T,Allocator>, *this, value);
        }
        void swap(array<T,Allocator>& r){
            std::swap(this->size_, r.size_);
            std::swap(this->allocator_.after->data, r.allocator_.after->data); // fixme
        }
        size_t size() const volatile {
            return size_;
        }
        bool empty() const volatile {
            return ((size() == 0) || bind::weak(*this));
        }
        value_type* data() volatile {
            return (value_type*)allocator_.data();
        }
        const value_type* data() const volatile {
            return (value_type*)allocator_.data();
        }
        value_type& operator[](size_t i) volatile {
            return data()[ i ];
        }
        const value_type& operator[](size_t i) const volatile {
            return data()[ i ];
        }
        value_type& at(size_type i) volatile {
            if(i >= size()) throw std::out_of_range("array::out_of_range");
            return (*this)[i];
        }
        const value_type& at(size_type i) const volatile {
            if(i >= size()) throw std::out_of_range("array::out_of_range");
            return (*this)[i];
        }
        value_type& front() volatile {
            return (*this)[0];
        }
        const value_type& front() const volatile {
            return (*this)[0];
        }
        value_type& back() volatile {
            return (*this)[size()-1];
        }
        const value_type& back() const volatile {
            return (*this)[size()-1];
        }
        iterator begin() volatile {
            return iterator(const_cast<array&>(*this), 0);
        }
        iterator end() volatile {
            return iterator(const_cast<array&>(*this), size());
        }
        const_iterator cbegin() const volatile {
            return const_iterator(const_cast<const array&>(*this), 0);
        }
        const_iterator cend() const volatile {
            return const_iterator(const_cast<const array&>(*this), size());
        }
        volatile_iterator vbegin() volatile {
            return volatile_iterator(*this, 0);
        }
        volatile_iterator vend() volatile {
            return volatile_iterator(*this, size());
        }
    private:
        mutable size_t size_;
    public:
        mutable allocator_type allocator_;
    };

}

#endif

#ifndef BIND_CONTAINER_BLOCK
#define BIND_CONTAINER_BLOCK

namespace bind {
     
    template<typename T, class Allocator = bind::snapshot> class block;
    namespace detail { 
        template<typename T>
        void fill_value(volatile block<T>& a, T& value){
            size_t size = a.num_rows()*a.num_cols();
            T* ad = a.data();
            for(size_t i = 0; i < size; ++i) ad[i] = value;
        }
    }

    template <class T, class Allocator>
    class block {
    public:
        typedef Allocator allocator_type;
        typedef T value_type;
        block(size_t m, size_t n) : allocator_(sizeof(T)*m*n), rows(m), cols(n) {}
        void init(T value){
            bind::cpu(detail::fill_value<T>, *this, value);
        }
        value_type& operator()(size_t i, size_t j){
            return data()[ j*rows + i ];
        }
        const value_type& operator()(size_t i, size_t j) const {
            return data()[ j*rows + i ];
        }
        value_type* data() volatile {
            return (value_type*)allocator_.data();
        }
        const value_type* data() const volatile {
            return (value_type*)allocator_.data();
        }
        size_t num_rows() const volatile {
            return rows;
        }
        size_t num_cols() const volatile {
            return cols;
        }
        size_t rows;
        size_t cols;
        mutable allocator_type allocator_;
    };

}

#endif
// }}}
#ifdef BIND_NO_DEBUG
#undef NDEBUG
#endif
#endif
