/*
 * Copyright Institute for Theoretical Physics, ETH Zurich 2015.
 * Distributed under the Boost Software License, Version 1.0.
 *
 * Permission is hereby granted, free of charge, to any person or organization
 * obtaining a copy of the software and accompanying documentation covered by
 * this license (the "Software") to use, reproduce, display, distribute,
 * execute, and transmit the Software, and to prepare derivative works of the
 * Software, and to permit third-parties to whom the Software is furnished to
 * do so, all subject to the following:
 *
 * The copyright notices in the Software and this entire statement, including
 * the above license grant, this restriction and the following disclaimer,
 * must be included in all copies of the Software, in whole or in part, and
 * all derivative works of the Software, unless such copies or derivative
 * works are solely in the form of machine-executable object code generated by
 * a source language processor.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
 * SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
 * FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
 * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
 * DEALINGS IN THE SOFTWARE.
 */

#ifndef BIND_MEMORY_HUB
#define BIND_MEMORY_HUB

namespace bind { namespace memory {

    template<device D>
    struct hub {
        static bool conserves(descriptor& c, descriptor& p){
            return (c.tmp || p.type == types::cpu::standard || c.type == types::cpu::bulk);
        }
        static void* malloc(descriptor& c){
            if(c.tmp || c.type == types::cpu::bulk){
                void* ptr = cpu::data_bulk::soft_malloc(c.extent);
                if(ptr){ c.type = types::cpu::bulk; return ptr; }
            }
            c.type = types::cpu::standard;
            return cpu::standard::malloc(c.extent);
        }
        static void memset(descriptor& desc, void* ptr){
            std::memset(ptr, 0, desc.extent);
        }
        static void memcpy(descriptor& dst_desc, void* dst, descriptor& src_desc, void* src){
            std::memcpy(dst, src, src_desc.extent);
        }
    };

    #ifdef CUDART_VERSION
    template<>
    struct hub<device::cpu> : public hub<device::any> {
        static bool is_sibling(descriptor& c){
            return c.type != types::gpu::standard;
        }
        static bool conserves(descriptor& c, descriptor& p){
            if(!is_sibling(p)) return false;
            return (c.tmp || p.type == types::cpu::standard || c.type == types::cpu::bulk);
        }
    };
    template<>
    struct hub<device::gpu> {
        static bool is_sibling(descriptor& c){
            return c.type == types::gpu::standard;
        }
        static bool conserves(descriptor& c, descriptor& p){
            return is_sibling(p);
        }
        static void* malloc(descriptor& c){
            c.type = types::gpu::standard;
            return gpu::standard::malloc(c.extent);
        }
        static void memset(descriptor& desc, void* ptr){
            cudaMemset(ptr, 0, desc.extent);
        }
        static void memcpy(descriptor& dst_desc, void* dst, descriptor& src_desc, void* src){
            cudaMemcpy(dst, src, src_desc.extent, cudaMemcpyDeviceToDevice);
        }
    };
    #endif
} }

#endif
